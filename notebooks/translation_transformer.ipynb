{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "GPU usage:\n",
      "  memory.used memory.free\n",
      "0       2 MiB   24266 MiB\n",
      "1       2 MiB   24266 MiB\n",
      "2       2 MiB   24266 MiB\n",
      "3   15209 MiB    9059 MiB\n",
      "4   15209 MiB    9059 MiB\n",
      "5    4033 MiB   20235 MiB\n",
      "Returning GPU0 with 24266 free MiB\n",
      "using GPU id: 0\n"
     ]
    }
   ],
   "source": [
    "# picking the most free GPU resource as cuda device\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output(\n",
    "        [\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]\n",
    "    )\n",
    "    gpu_df = pd.read_csv(\n",
    "        StringIO(gpu_stats.decode(\"utf-8\")),\n",
    "        names=[\"memory.used\", \"memory.free\"],\n",
    "        skiprows=1,\n",
    "    )\n",
    "    print(\"GPU usage:\\n{}\".format(gpu_df))\n",
    "    gpu_df[\"memory.free\"] = gpu_df[\"memory.free\"].map(\n",
    "        lambda x: int(x.rstrip(\" MiB\"))\n",
    "    )\n",
    "    idx = gpu_df[\"memory.free\"].idxmax()\n",
    "    print(\n",
    "        \"Returning GPU{} with {} free MiB\".format(\n",
    "            idx, gpu_df.iloc[idx][\"memory.free\"]\n",
    "        )\n",
    "    )\n",
    "    return idx\n",
    "\n",
    "\n",
    "cmd = \"export CUDA_VISIBLE_DEVICES=1,2,3,4,5,6\"\n",
    "os.popen(cmd)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    free_gpu_id = get_free_gpu()\n",
    "    print(f\"using GPU id: {free_gpu_id}\")\n",
    "    torch.cuda.set_device(free_gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Language Translation with nn.Transformer and torchtext\n",
    "======================================================\n",
    "\n",
    "This tutorial shows, how to train a translation model from scratch using\n",
    "Transformer. We will be using `Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>`__ \n",
    "dataset to train a German to English translation model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sourcing and Processing\n",
    "----------------------------\n",
    "\n",
    "`torchtext library <https://pytorch.org/text/stable/>`__ has utilities for creating datasets that can be easily\n",
    "iterated through for the purposes of creating a language translation\n",
    "model. In this example, we show how to use torchtext's inbuilt datasets, \n",
    "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
    "`Multi30k dataset from torchtext library <https://pytorch.org/text/stable/datasets.html#multi30k>`__\n",
    "that yields a pair of source-target raw sentences. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.datasets_utils import _RawTextIterableDataset\n",
    "\n",
    "\n",
    "def get_multiwoz(split=(\"train\", \"val\", \"test\")):\n",
    "    files = {\n",
    "        \"train\": \"../data/train.history_belief\",\n",
    "        \"val\": \"../data/val.history_belief\",\n",
    "        \"test\": \"../data/test.history_belief\",\n",
    "    }\n",
    "    datas = []\n",
    "    for name in split:\n",
    "        with open(files[name]) as fp:\n",
    "            raw_text = fp.read()\n",
    "            raw_text = raw_text.replace(\n",
    "                \"<|endofcontext|>\", \" <|endofcontext|>\"\n",
    "            )\n",
    "            raw_text = raw_text.replace(\"<|endoftext|>\", \"\")\n",
    "            texts = raw_text.split(\"\\n\")\n",
    "\n",
    "            data = []\n",
    "\n",
    "            for text in texts:\n",
    "                if not text.split():\n",
    "                    continue\n",
    "                split_index = text.find(\"<|belief|>\")\n",
    "                input_text = text[: split_index - 1]\n",
    "                belief = text[split_index:]\n",
    "                data.append((input_text, belief))\n",
    "            datas.append(\n",
    "                _RawTextIterableDataset(\"MULTIWOZ\", len(data), iter(data))\n",
    "            )\n",
    "\n",
    "    return (data for data in datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = get_multiwoz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "def split_iterable_dataset(dataset, ratio=0.5):\n",
    "    n = int(len(dataset) * ratio)\n",
    "    samples = sample(list(dataset), n)\n",
    "    return (\n",
    "        _RawTextIterableDataset(\"MULTIWOZ\", len(samples), iter(samples)),\n",
    "        samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsize_train_iter, downsize_train_data = split_iterable_dataset(\n",
    "    train_set, 1\n",
    ")\n",
    "downsize_val_iter, downsize_val_data = split_iterable_dataset(val_set, 0.01)\n",
    "test_data = list(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "    for data_sample in data_iter:\n",
    "        yield data_sample[0].split() + data_sample[1].split()\n",
    "\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX = 0, 1\n",
    "\n",
    "INPUT_SOS = \"<|context|>\"\n",
    "INPUT_EOS = \"<|endofcontext|>\"\n",
    "OUTPUT_SOS = \"<|belief|>\"\n",
    "OUTPUT_EOS = \"<|endofbelief|>\"\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(downsize_train_iter),\n",
    "    min_freq=1,\n",
    "    specials=special_symbols,\n",
    "    special_first=True,\n",
    ")\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "OUTPUT_EOS_IDX = vocab([OUTPUT_EOS])[0]\n",
    "OUTPUT_SOS_IDX = vocab([OUTPUT_SOS])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq Network using Transformer\n",
    "---------------------------------\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
    "need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
    "paper for solving machine translation tasks. \n",
    "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
    "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
    "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
    "encodings to provide position information of input tokens to the model. The second part is the \n",
    "actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model. \n",
    "Finally, the output of Transformer model is passed through linear layer\n",
    "that give un-normalized probabilities for each token in the target language. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(\n",
    "            -torch.arange(0, emb_size, 2) * math.log(10000) / emb_size\n",
    "        )\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(\n",
    "            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n",
    "        )\n",
    "\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        trg: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        tgt_mask: Tensor,\n",
    "        src_padding_mask: Tensor,\n",
    "        tgt_padding_mask: Tensor,\n",
    "        memory_key_padding_mask: Tensor,\n",
    "    ):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            None,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            memory_key_padding_mask,\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
    "        )\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need a subsequent word mask that will prevent model to look into\n",
    "the future words when making predictions. We will also need masks to hide\n",
    "source and target padding tokens. Below, let's define a function that will take care of both. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device=torch.device(\"cpu\")):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(\n",
    "        0, 1\n",
    "    )\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, device=torch.device(\"cpu\")):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device=device)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(\n",
    "        torch.bool\n",
    "    )\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_to_state_list(belief):\n",
    "    if OUTPUT_EOS in belief:\n",
    "        first_idx_eos = belief.index(OUTPUT_EOS)\n",
    "        belief = belief[:first_idx_eos]\n",
    "    belief_list = [\n",
    "        token for token in belief if token not in [OUTPUT_SOS, OUTPUT_EOS]\n",
    "    ]\n",
    "    belief_list = [slot.split() for slot in \" \".join(belief_list).split(\",\")]\n",
    "    return belief_list\n",
    "\n",
    "\n",
    "def belief_to_state_dict(belief):\n",
    "    belief_list = belief_to_state_list(belief)\n",
    "    state_dict = {}\n",
    "    for state in belief_list:\n",
    "        if len(state) < 3:\n",
    "            continue\n",
    "        domain = state[0]\n",
    "        slot = state[1]\n",
    "        sub_slot = None\n",
    "        rest = state[2:]\n",
    "        if slot == \"book\":\n",
    "            sub_slot = state[2]\n",
    "            rest = state[3:]\n",
    "        value = \" \".join(rest)\n",
    "        d = state_dict.get(domain, {})\n",
    "        if sub_slot:\n",
    "            ss = d.get(slot, {})\n",
    "            ss.update({sub_slot: value})\n",
    "            d.update({slot: ss})\n",
    "        else:\n",
    "            d.update({slot: value})\n",
    "        state_dict.update({domain: d})\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def match_slot(true, pred):\n",
    "    pred_state = belief_to_state_dict(pred)\n",
    "    true_list = belief_to_state_list(true)\n",
    "    slot_matches = []\n",
    "    for i, state in enumerate(true_list):\n",
    "        slot_matches.append(False)\n",
    "        if len(state) < 3:\n",
    "            continue\n",
    "        domain = state[0]\n",
    "        if domain not in pred_state.keys():\n",
    "            continue\n",
    "\n",
    "        slot = state[1]\n",
    "        if slot not in pred_state[domain].keys():\n",
    "            continue\n",
    "\n",
    "        if slot != \"book\":\n",
    "            true_value = \" \".join(state[2:])\n",
    "            pred_value = pred_state[domain][slot]\n",
    "        else:\n",
    "            sub_slot = state[2]\n",
    "            if sub_slot not in pred_state[domain][slot]:\n",
    "                continue\n",
    "            true_value = \" \".join(state[3:])\n",
    "            pred_value = pred_state[domain][slot][sub_slot]\n",
    "\n",
    "        if true_value != pred_value:\n",
    "            continue\n",
    "        slot_matches[i] = True\n",
    "\n",
    "    all_match = sum(slot_matches) == len(true_list)\n",
    "\n",
    "    return all_match, slot_matches\n",
    "\n",
    "\n",
    "def get_accuracy(results):\n",
    "    total_states = len(results)\n",
    "    total_slots = sum([len(result[1]) for result in results])\n",
    "    total_correct_states = sum([result[0] for result in results])\n",
    "    total_correct_slots = sum([sum(result[1]) for result in results])\n",
    "    return {\n",
    "        \"joint_accuracy\": total_correct_states / total_states,\n",
    "        \"slot_accuracy\": total_correct_slots / total_slots,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
    "define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab)\n",
    "TGT_VOCAB_SIZE = len(vocab)\n",
    "EMB_SIZE = 128\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMB_SIZE,\n",
    "    NHEAD,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM,\n",
    ")\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collation\n",
    "---------\n",
    "\n",
    "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
    "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
    "defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
    "can be fed directly into our model.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return torch.tensor(txt_input)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform_func = sequential_transforms(\n",
    "    lambda s: s.split(), vocab,  # Tokenization  # Numericalization\n",
    ")\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform_func(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform_func(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define training and evaluation loop that will be called for each \n",
    "epoch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    # (train_iter,) = get_multiwoz(split=(\"train\",))\n",
    "    train_dataloader = DataLoader(\n",
    "        downsize_train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            src, tgt_input, device=device\n",
    "        )\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(\n",
    "            logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    val_model = model.to(torch.device(\"cpu\"))\n",
    "    # (val_iter,) = get_multiwoz(split=(\"val\",))\n",
    "    val_dataloader = DataLoader(\n",
    "        downsize_val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(torch.device(\"cpu\"))\n",
    "        tgt = tgt.to(torch.device(\"cpu\"))\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            src, tgt_input, device=torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "        logits = val_model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(\n",
    "            logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)\n",
    "        )\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the ingredients to train our model. Let's do it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.451, Val loss: 0.896, Epoch time = 74.265s\n",
      "Epoch: 2, Train loss: 0.700, Val loss: 0.506, Epoch time = 77.155s\n",
      "Epoch: 3, Train loss: 0.529, Val loss: 0.432, Epoch time = 77.132s\n",
      "Epoch: 4, Train loss: 0.451, Val loss: 0.380, Epoch time = 78.372s\n",
      "Epoch: 5, Train loss: 0.389, Val loss: 0.329, Epoch time = 77.983s\n",
      "Epoch: 6, Train loss: 0.322, Val loss: 0.257, Epoch time = 77.302s\n",
      "Epoch: 7, Train loss: 0.265, Val loss: 0.223, Epoch time = 78.123s\n",
      "Epoch: 8, Train loss: 0.227, Val loss: 0.190, Epoch time = 77.783s\n",
      "Epoch: 9, Train loss: 0.189, Val loss: 0.166, Epoch time = 77.991s\n",
      "Epoch: 10, Train loss: 0.159, Val loss: 0.154, Epoch time = 76.898s\n",
      "Epoch: 11, Train loss: 0.137, Val loss: 0.155, Epoch time = 76.953s\n",
      "Epoch: 12, Train loss: 0.122, Val loss: 0.149, Epoch time = 77.256s\n",
      "Epoch: 13, Train loss: 0.110, Val loss: 0.148, Epoch time = 78.062s\n",
      "Epoch: 14, Train loss: 0.100, Val loss: 0.148, Epoch time = 77.718s\n",
      "Epoch: 15, Train loss: 0.092, Val loss: 0.154, Epoch time = 78.383s\n",
      "Epoch: 16, Train loss: 0.086, Val loss: 0.146, Epoch time = 76.839s\n",
      "Epoch: 17, Train loss: 0.080, Val loss: 0.155, Epoch time = 77.855s\n",
      "Epoch: 18, Train loss: 0.074, Val loss: 0.153, Epoch time = 78.375s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d04cd72534b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-df0f7ddb6f5a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCHS = 60\n",
    "min_val_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epoches = []\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = timer()\n",
    "    transformer = transformer.to(device)\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    torch.cuda.empty_cache()\n",
    "    transformer = transformer.to(torch.device(\"cuda\"))\n",
    "    val_loss = evaluate(transformer)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": transformer.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": train_loss,\n",
    "            },\n",
    "            \"./transformer_checkpoint\",\n",
    "        )\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )\n",
    "    epoches.append(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    test_model = model.to(torch.device(\"cpu\"))\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "    results = []\n",
    "    for src, tgt in tqdm(test_dataloader):\n",
    "        src = src.to(torch.device(\"cpu\"))\n",
    "        \n",
    "        num_tokens = src.shape[0]\n",
    "        \n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(torch.device('cpu'))\n",
    "        memory = model.encode(src, src_mask)\n",
    "        \n",
    "        tgt = tgt.to(torch.device(\"cpu\"))\n",
    "\n",
    "        start_symbol = OUTPUT_SOS_IDX\n",
    "        eos_bools = torch.zeros(src.size(1), dtype=torch.bool)\n",
    "        top_indices = torch.ones(1, src.size(1)).fill_(start_symbol).type(torch.long).to(torch.device('cpu'))\n",
    "        max_len = 128\n",
    "        for i in range(max_len - 1):\n",
    "            tgt_mask = (\n",
    "                generate_square_subsequent_mask(top_indices.size(0)).type(torch.bool)\n",
    "            ).to(torch.device('cpu'))\n",
    "            out = model.decode(top_indices, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            prob = model.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.view(1, next_word.size(0))\n",
    "            eos_bools = (next_word == OUTPUT_EOS_IDX).logical_or(eos_bools) \n",
    "            top_indices = torch.cat(\n",
    "                [top_indices, next_word], dim=0\n",
    "            )\n",
    "            if torch.sum(eos_bools).item() == eos_bools.size(1):\n",
    "                break\n",
    "        # keep most likely tokens\n",
    "        \n",
    "        for idx in range(top_indices.shape[1]):\n",
    "            pred = vocab.lookup_tokens(list(top_indices[:,idx].cpu().numpy()))\n",
    "            true = vocab.lookup_tokens(list(tgt[:,idx].cpu().numpy()))\n",
    "            \n",
    "            result = match_slot(true, pred)\n",
    "            results.append(result)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./transformer_checkpoint\")\n",
    "transformer.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [07:03<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "transformer = transformer.to(torch.device('cpu'))\n",
    "results = test(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_accuracy': 0.04395008138903961, 'slot_accuracy': 0.6080988652261775}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(\n",
    "    model, src, src_mask, max_len, start_symbol, device=torch.device(\"cpu\")\n",
    "):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len - 1):\n",
    "        memory = memory.to(device)\n",
    "        tgt_mask = (\n",
    "            generate_square_subsequent_mask(ys.size(0)).type(torch.bool)\n",
    "        ).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0\n",
    "        )\n",
    "        if next_word == OUTPUT_EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform_func(src_sentence).view(-1, 1)\n",
    "    # print(src)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model, src, src_mask, max_len=100, start_symbol=OUTPUT_SOS_IDX\n",
    "    ).flatten()\n",
    "    return vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|belief|>', 'train', 'leaveat', '12:45', ',', 'train', 'destination', 'peterborough', ',', 'train', 'day', 'wednesday', ',', 'train', 'arriveby', 'not', 'mentioned', ',', 'train', 'departure', 'cambridge', '<|endofbelief|>']\n"
     ]
    }
   ],
   "source": [
    "transformer = transformer.to(torch.device(\"cpu\"))\n",
    "torch.cuda.empty_cache()\n",
    "print(\n",
    "    translate(\n",
    "        transformer,\n",
    "        \"<|context|> <|user|> i need to take a train out of cambridge , i will be leaving town on wednesday . <|system|> there are 5 trains out of cambridge on wednesday . do you have a departure time in mind ? <|user|> i would like to go to peterborough and leave after 12:45 , i have to attend a meeting beforehand . <|system|> tr1879 leaves at 13:06 on wednesday . will that work for you ? <|user|> what is the price of the fair and could you tell me what is the arrival time into peterborough ?<|endofcontext|>\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <|context|> <|user|> i want to find a moderate -ly priced restaurant . <|system|> i have many options available for you ! is there a certain area or cuisine that interests you ? <|user|> yes i would like the restaurant to be located in the center of the attractions . <|system|> there are 21 restaurant -s available in the centre of town . how about a specific type of cuisine ? <|user|> i need to know the food type and postcode and it should also have mutliple sports <|system|> i am sorry i do not understand what you just said . please repeat in a way that makes sense . <|user|> get me the food type and the post code <|endofcontext|>\n",
      "['<|belief|>', 'restaurant', 'food', 'not', 'mentioned', ',', 'restaurant', 'pricerange', 'moderate', ',', 'restaurant', 'name', 'not', 'mentioned', ',', 'restaurant', 'area', 'centre', '<|endofbelief|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, [True, True, True, True])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample, output_sample = test_data[17]\n",
    "pred = translate(\n",
    "        transformer,\n",
    "        input_sample,\n",
    "    )\n",
    "print(input_sample)\n",
    "print(pred)\n",
    "match_slot(output_sample.split(), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for x, y in tqdm(test_data):\n",
    "    pred = translate(transformer, x)\n",
    "    result = match_slot(y.split(), pred)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_accuracy': 0.0, 'slot_accuracy': 0.021744886992009223}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "1. Attention is all you need paper.\n",
    "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "language": "python",
   "name": "python3812jvsc74a57bd04a64bfe93dbb51924a5ad8d7e9312f968a417a773494ea590cc93068d261664b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
