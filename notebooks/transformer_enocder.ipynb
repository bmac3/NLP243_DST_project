{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64c3624-70d2-41e1-8bba-3b1ce0e2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# picking the most free GPU resource as cuda device\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output(\n",
    "        [\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]\n",
    "    )\n",
    "    gpu_df = pd.read_csv(\n",
    "        StringIO(gpu_stats.decode(\"utf-8\")),\n",
    "        names=[\"memory.used\", \"memory.free\"],\n",
    "        skiprows=1,\n",
    "    )\n",
    "    print(\"GPU usage:\\n{}\".format(gpu_df))\n",
    "    gpu_df[\"memory.free\"] = gpu_df[\"memory.free\"].map(\n",
    "        lambda x: int(x.rstrip(\" MiB\"))\n",
    "    )\n",
    "    idx = gpu_df[\"memory.free\"].idxmax()\n",
    "    print(\n",
    "        \"Returning GPU{} with {} free MiB\".format(\n",
    "            idx, gpu_df.iloc[idx][\"memory.free\"]\n",
    "        )\n",
    "    )\n",
    "    return idx\n",
    "\n",
    "\n",
    "cmd = \"export CUDA_VISIBLE_DEVICES=1,2,3,4,5,6\"\n",
    "os.popen(cmd)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    free_gpu_id = get_free_gpu()\n",
    "    print(f\"using GPU id: {free_gpu_id}\")\n",
    "    torch.cuda.set_device(free_gpu_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539ace1c-6e23-41b2-93bb-2c27c6c7a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d7084a-36ce-4df6-82fc-eadf03524948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "with open('../data/train.history_belief') as fp:\n",
    "    raw_train_data = [line.split() for line in fp.read().split('\\n')]\n",
    "    \n",
    "with open('../data/val.history_belief') as fp:\n",
    "    raw_val_data = [line.split() for line in fp.read().split('\\n')][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b3a76d-0c08-4943-8dc4-c340c5fe83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = torchtext.vocab.build_vocab_from_iterator(raw_train_data, specials=[\"<unk>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0865e2-bd47-4f03-997f-8709be79b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.set_default_index(train_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53565f97-e1ce-4bed-92f8-925b4e53d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882f3874-21ee-4498-a1de-555fd2a2dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [torch.tensor(train_vocab(sent)[::-1], dtype=torch.long) for sent in raw_train_data]\n",
    "val_data = [torch.tensor(train_vocab(sent)[::-1], dtype=torch.long) for sent in raw_val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c842d369-379a-4024-8802-2935e14dc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequence(train_data, padding_value=train_vocab['<pad>'])\n",
    "val_data = pad_sequence(val_data, padding_value=train_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4a16d3-f80c-4af3-ae14-665048a1138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.flip(train_data, (0,))\n",
    "val_data = torch.flip(val_data, (0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f97eab-e96d-4db1-b7f5-9359d38f5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    ntoken=len(train_vocab),\n",
    "    ninp=50,\n",
    "    nhead=2,\n",
    "    nhid=50,\n",
    "    nlayers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "df7d6ad4-522d-40f1-b640-95e74723755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [01:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-357-7f79e59b7d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-710e95e6f4bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, has_mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 20\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "total_loss = 0.\n",
    "start_time = time.time()\n",
    "ntokens = len(train_vocab)\n",
    "min_val_loss = math.inf\n",
    "for _ in tqdm(range(n_epochs), total=n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx in range(0, len(train_data), batch_size):\n",
    "        data = train_data[:, batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        data, targets = data[:-1, :].T, data[1:, :].T\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.transpose(2, 1), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss / 10\n",
    "            elapsed = time.time() - start_time\n",
    "            print('ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                batch,\n",
    "                elapsed * 1000 / 10, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    model.eval()\n",
    "    out = model(val_data[:-1, :].T)\n",
    "    val_loss = criterion(out.transpose(2, 1), val_data[1:, :].T)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        lr /= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2b7b4c5-9169-458e-96f2-798ffbe2e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test.history_belief') as fp:\n",
    "    raw_test_data = [line.split() for line in fp.read().split('\\n')][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e498af-eb6f-4ccc-83de-13d9d43d2daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EOT = '<|endoftext|>'\n",
    "\n",
    "def translate(indexes):\n",
    "    indexes = list(indexes)\n",
    "    try:\n",
    "        found = indexes.index(EOT)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    else:\n",
    "        indexes = indexes[:found+1]\n",
    "    return train_vocab.lookup_tokens(list(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "212e5604-6681-499c-8aac-37c56b9da8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SOS = '<|context|>'\n",
    "INPUT_EOS = '<|endofcontext|>'\n",
    "OUTPUT_SOS = '<|belief|>'\n",
    "OUTPUT_EOS = '<|endofbelief|>'\n",
    "\n",
    "def belief_to_state_list(belief):\n",
    "    belief_list = [token for token in belief if token not in [OUTPUT_SOS, OUTPUT_EOS]]\n",
    "    belief_list = [slot.split() for slot in ' '.join(belief_list).split(',')]\n",
    "    return belief_list\n",
    "\n",
    "def belief_to_state_dict(belief):\n",
    "    belief_list = belief_to_state_list(belief)\n",
    "    state_dict = {}\n",
    "    for state in belief_list:\n",
    "        if len(state) < 3: continue\n",
    "        domain = state[0]\n",
    "        slot = state[1]\n",
    "        sub_slot = None\n",
    "        rest = state[2:]\n",
    "        if slot == 'book':\n",
    "            sub_slot = state[2]\n",
    "            rest = state[3:]\n",
    "        value = ' '.join(rest)\n",
    "        d = state_dict.get(domain, {})\n",
    "        if sub_slot:\n",
    "            ss = d.get(slot, {})\n",
    "            ss.update({\n",
    "                sub_slot: value\n",
    "            })\n",
    "            d.update({slot: ss})\n",
    "        else:\n",
    "            d.update({slot: value})\n",
    "        state_dict.update({domain: d})\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def match_slot(true, pred):\n",
    "    pred_state = belief_to_state_dict(pred)\n",
    "    true_list = belief_to_state_list(true)\n",
    "    slot_matches = []\n",
    "    for i, state in enumerate(true_list):\n",
    "        if len(state) < 3: continue\n",
    "        slot_matches.append(False)\n",
    "        domain = state[0]\n",
    "        if domain not in pred_state.keys(): continue\n",
    "        \n",
    "        slot = state[1]\n",
    "        if slot not in pred_state[domain].keys(): continue\n",
    "        \n",
    "        if slot != 'book':\n",
    "            true_value = \" \".join(state[2:])\n",
    "            pred_value = pred_state[domain][slot]\n",
    "        else:\n",
    "            sub_slot = state[2]\n",
    "            if sub_slot not in pred_state[domain][slot]: continue\n",
    "            true_value = \" \".join(state[3:])\n",
    "            pred_value = pred_state[domain][slot][sub_slot]\n",
    "        \n",
    "        if true_value != pred_value: continue\n",
    "        slot_matches[i] = True\n",
    "            \n",
    "    all_match = sum(slot_matches) == len(true_list)\n",
    "    \n",
    "    return all_match, slot_matches\n",
    "\n",
    "def get_accuracy(results):\n",
    "    total_states = len(results)\n",
    "    total_slots = sum([len(result[1]) for result in results])\n",
    "    total_correct_states = sum([result[0] for result in results])\n",
    "    total_correct_slots = sum([sum(result[1]) for result in results])\n",
    "    return {\n",
    "        'joint_accuracy': total_correct_states / total_states,\n",
    "        'slot_accuracy': total_correct_slots / total_slots\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e81ed3f-4e01-43d2-affe-ca347cfddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9faedef-a592-4290-a0af-c0011f447740",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELIEF = '<|belief|>'\n",
    "\n",
    "test_data = []\n",
    "for raw_sent in raw_test_data[:-1]:\n",
    "    indexes = train_vocab(raw_sent)\n",
    "    belief_idx = indexes.index(train_vocab[BELIEF])\n",
    "    data, target = indexes[:belief_idx][::-1], indexes[belief_idx:]\n",
    "    test_data.append((torch.tensor(data, dtype=torch.long),\n",
    "                     torch.tensor(target, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628e3fe4-f487-4cb2-aae2-1f9ad3f57c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = pad_sequence([t[0] for t in test_data], padding_value=train_vocab['<pad>'])\n",
    "test_inputs = torch.flip(test_inputs, dims=(0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5820196a-c8ab-4f30-89ad-35cdc5038dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beliefs = [t[1] for t in test_data]\n",
    "max_test_target_len = max(len(t[1]) for t in test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ca073cc-e4a7-4e1c-8696-19546e4267f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.67it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = [torch.ones(test_inputs.shape[1], dtype=torch.long) * train_vocab[BELIEF]]\n",
    "input_data = torch.cat([test_inputs, torch.ones(1, test_inputs.shape[1], dtype=torch.long) * train_vocab[BELIEF]]).to(device)\n",
    "model.eval()\n",
    "for _ in tqdm(range(max_test_target_len), total=max_test_target_len):\n",
    "    out = model(input_data)\n",
    "    predictions.append(torch.argmax(out[:, -1, :], axis=1))\n",
    "    input_data = torch.cat([input_data, predictions[-1].unsqueeze(dim=1)], dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73af9d29-ca66-43b7-ac17-54f677230ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4] at entry 0 and [103] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-11a833e20d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4] at entry 0 and [103] at entry 1"
     ]
    }
   ],
   "source": [
    "predictions = torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4726326-f5e4-4d90-ba88-c2eb75396365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb0fa7f-1189-4ea5-baf5-c2c4deac188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5558b40f-468e-47d8-820a-17ddfa8eecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11463,  9129,  6649,  1465, 11496,  2222,  6903, 10668,  8855, 12591,\n",
       "         6196,  2544,  4427, 15523, 13777,  5669,  4628,  5228,   207,  6762,\n",
       "         9129,  4154,  3011,  7340,   998,   998, 11208,  5522, 15210,  6268,\n",
       "        15582, 15582,  8719, 11208, 12248, 13677,  7950,  7745, 11208, 12473,\n",
       "         1875,  1819,  1931,  2222,   334,  1931,  1931, 11347,  1931, 11017,\n",
       "         1931,  8945,  5033, 13411,   206, 15228,  4215, 11496,   443,  3471,\n",
       "         1022, 13677, 13777,  7711,  1022, 13777, 12694, 13777,  7021, 15920,\n",
       "          334,  7021, 13677, 13442,  1192,   334,   803,  9814,   334,   334,\n",
       "        10814, 10814,  9239, 15920, 11016, 11507,  7443, 15847, 11496,  8405,\n",
       "         8405,  7207,  9916,  9916,   334,  9916,  8182,  7021, 10207,   334,\n",
       "        13442,  4154, 15210])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1cf5b8a-0ece-45f4-8fa5-f556cb15b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 29])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([input_data, predictions[-1].unsqueeze(dim=1)], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc1b15c0-7a4c-4d4a-8e45-604558a480c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 27, 16070])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9521cc65-9745-4643-8762-17e44128c689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out[:, -1, :], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1aa19d87-0a56-41b3-b9af-289559dcdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.cat([test_inputs, torch.ones(1, test_inputs.shape[1], dtype=torch.long) * train_vocab[BELIEF]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "999a999b-c210-4acc-8088-5925bf8a6b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16bf1b0-5393-498e-92f4-f0b94664db9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103, 4, 16070])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0724a-93f1-4ec8-9e29-a860e485c134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4d28a-4aae-49d6-8375-cadcc29e7213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebf850-e873-4796-859c-38a31bc3e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "a33425be-4100-4445-816f-6ef997f5e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for pred, target in zip(predictions.T, test_beliefs):\n",
    "    result = match_slot(translate(target), translate(pred))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "a809e16f-2bb7-4b2a-b911-4627dcbe7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_accuracy': 0.0, 'slot_accuracy': 0.0}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c43d2-de66-4ffa-9451-c33c6f74a1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP220",
   "language": "python",
   "name": "nlp220"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
