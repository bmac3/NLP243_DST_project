{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "539ace1c-6e23-41b2-93bb-2c27c6c7a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d7084a-36ce-4df6-82fc-eadf03524948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "with open('../data/train.history_belief') as fp:\n",
    "    raw_train_data = [line.split() for line in fp.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b3a76d-0c08-4943-8dc4-c340c5fe83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = torchtext.vocab.build_vocab_from_iterator(raw_train_data, specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0865e2-bd47-4f03-997f-8709be79b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.set_default_index(train_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "882f3874-21ee-4498-a1de-555fd2a2dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [torch.tensor([train_vocab(sent)], dtype=torch.long) for sent in raw_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58f97eab-e96d-4db1-b7f5-9359d38f5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    ntoken=len(train_vocab),\n",
    "    ninp=512,\n",
    "    nhead=2,\n",
    "    nhid=200,\n",
    "    nlayers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df7d6ad4-522d-40f1-b640-95e74723755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:02,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 10.00 | loss 187.38 | ppl     8.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:04,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 20.00 | loss 198.77 | ppl     7.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:06,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 30.00 | loss 254.44 | ppl     7.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 40.00 | loss 247.18 | ppl     6.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:11,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 50.00 | loss 216.02 | ppl     6.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:12,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 60.00 | loss 159.10 | ppl     5.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:15,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 70.00 | loss 268.92 | ppl     5.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [00:17,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 80.00 | loss 233.55 | ppl     5.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:19,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 90.00 | loss 203.82 | ppl     4.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:22,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 100.00 | loss 281.05 | ppl     4.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:24,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms/batch 110.00 | loss 238.35 | ppl     4.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:25,  4.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-04a00d89116d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 20\n",
    "\n",
    "model.train()\n",
    "total_loss = 0.\n",
    "start_time = time.time()\n",
    "ntokens = len(train_vocab)\n",
    "for batch, sent in tqdm(enumerate(train_data)):\n",
    "    data, targets = sent[:, :-1], sent[:, 1:]\n",
    "    model.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output[0], targets[0])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(p.grad, alpha=-lr)\n",
    "    total_loss += loss.item()\n",
    "    if batch % 10 == 0 and batch > 0:\n",
    "        cur_loss = total_loss / 10\n",
    "        elapsed = time.time() - start_time\n",
    "        print('ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "            batch,\n",
    "            elapsed * 1000 / 10, cur_loss, math.exp(cur_loss)))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b7b4c5-9169-458e-96f2-798ffbe2e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/val.history_belief') as fp:\n",
    "    raw_val_data = [line.split() for line in fp.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5711443-472d-44ba-bbe7-2bc807450b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELIEF = '<|belief|>'\n",
    "\n",
    "val_data = []\n",
    "for raw_sent in raw_val_data[:-1]:\n",
    "    indexes = train_vocab(raw_sent)\n",
    "    belief_idx = indexes.index(train_vocab[BELIEF])\n",
    "    data, target = indexes[:belief_idx], indexes[belief_idx:]\n",
    "    val_data.append((torch.tensor(data, dtype=torch.long),\n",
    "                     torch.tensor(target, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6e498af-eb6f-4ccc-83de-13d9d43d2daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(indexes):\n",
    "    return train_vocab.lookup_tokens(list(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "212e5604-6681-499c-8aac-37c56b9da8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SOS = '<|context|>'\n",
    "INPUT_EOS = '<|endofcontext|>'\n",
    "OUTPUT_SOS = '<|belief|>'\n",
    "OUTPUT_EOS = '<|endofbelief|>'\n",
    "\n",
    "def belief_to_state_list(belief):\n",
    "    belief_list = [token for token in belief if token not in [OUTPUT_SOS, OUTPUT_EOS]]\n",
    "    belief_list = [slot.split() for slot in ' '.join(belief_list).split(',')]\n",
    "    return belief_list\n",
    "\n",
    "def belief_to_state_dict(belief):\n",
    "    belief_list = belief_to_state_list(belief)\n",
    "    state_dict = {}\n",
    "    for state in belief_list:\n",
    "        if len(state) < 3: continue\n",
    "        domain = state[0]\n",
    "        slot = state[1]\n",
    "        sub_slot = None\n",
    "        rest = state[2:]\n",
    "        if slot == 'book':\n",
    "            sub_slot = state[2]\n",
    "            rest = state[3:]\n",
    "        value = ' '.join(rest)\n",
    "        d = state_dict.get(domain, {})\n",
    "        if sub_slot:\n",
    "            ss = d.get(slot, {})\n",
    "            ss.update({\n",
    "                sub_slot: value\n",
    "            })\n",
    "            d.update({slot: ss})\n",
    "        else:\n",
    "            d.update({slot: value})\n",
    "        state_dict.update({domain: d})\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def match_slot(true, pred):\n",
    "    pred_state = belief_to_state_dict(pred)\n",
    "    true_list = belief_to_state_list(true)\n",
    "    slot_matches = []\n",
    "    for i, state in enumerate(true_list):\n",
    "        if len(state) < 3: continue\n",
    "        slot_matches.append(False)\n",
    "        domain = state[0]\n",
    "        if domain not in pred_state.keys(): continue\n",
    "        \n",
    "        slot = state[1]\n",
    "        if slot not in pred_state[domain].keys(): continue\n",
    "        \n",
    "        if slot != 'book':\n",
    "            true_value = \" \".join(state[2:])\n",
    "            pred_value = pred_state[domain][slot]\n",
    "        else:\n",
    "            sub_slot = state[2]\n",
    "            if sub_slot not in pred_state[domain][slot]: continue\n",
    "            true_value = \" \".join(state[3:])\n",
    "            pred_value = pred_state[domain][slot][sub_slot]\n",
    "        \n",
    "        if true_value != pred_value: continue\n",
    "        slot_matches[i] = True\n",
    "            \n",
    "    all_match = sum(slot_matches) == len(true_list)\n",
    "    \n",
    "    return all_match, slot_matches\n",
    "\n",
    "def get_accuracy(results):\n",
    "    total_states = len(results)\n",
    "    total_slots = sum([len(result[1]) for result in results])\n",
    "    total_correct_states = sum([result[0] for result in results])\n",
    "    total_correct_slots = sum([sum(result[1]) for result in results])\n",
    "    return {\n",
    "        'joint_accuracy': total_correct_states / total_states,\n",
    "        'slot_accuracy': total_correct_slots / total_slots\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e81ed3f-4e01-43d2-affe-ca347cfddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32cabdab-a1d3-4e41-9a06-38453cc7d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "EOT = '<|endoftext|>'\n",
    "\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "for data, target in tqdm(val_data[:2]):\n",
    "    prediction = [train_vocab[BELIEF]]\n",
    "    input_data = torch.cat([data, torch.tensor([train_vocab[BELIEF]], dtype=torch.long)])\n",
    "    while len(prediction) < 5 and prediction[-1] != EOT:\n",
    "        out = model(input_data)\n",
    "        pred = torch.argmax(out[0, -1])\n",
    "        prediction.append(int(pred))\n",
    "        input_data = torch.cat([input_data, pred.reshape(1)])\n",
    "    predictions.append(prediction)\n",
    "    result = match_slot(translate(target), translate(prediction))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "803d2c3b-1e3f-4780-89e0-6395291d9f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_accuracy': 0.0, 'slot_accuracy': 0.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13520c2e-c617-4015-863b-a1fdc3d67abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d436eb-2c7f-4ad6-96aa-6cd3957aaf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181056c-f5e1-4bf4-bdb5-751b5926f536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1fac3-e05f-44f2-8d45-0464844c9f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b6111-7350-4a3f-846a-ac5553052590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP220",
   "language": "python",
   "name": "nlp220"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
