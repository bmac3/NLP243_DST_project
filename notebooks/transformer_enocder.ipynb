{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64c3624-70d2-41e1-8bba-3b1ce0e2282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "GPU usage:\n",
      "  memory.used memory.free\n",
      "0   14745 MiB    9523 MiB\n",
      "1    8571 MiB   15697 MiB\n",
      "2   14640 MiB    9628 MiB\n",
      "3   23780 MiB     488 MiB\n",
      "4   23780 MiB     488 MiB\n",
      "5   12604 MiB   11664 MiB\n",
      "Returning GPU1 with 15697 free MiB\n",
      "using GPU id: 1\n"
     ]
    }
   ],
   "source": [
    "# picking the most free GPU resource as cuda device\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output(\n",
    "        [\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]\n",
    "    )\n",
    "    gpu_df = pd.read_csv(\n",
    "        StringIO(gpu_stats.decode(\"utf-8\")),\n",
    "        names=[\"memory.used\", \"memory.free\"],\n",
    "        skiprows=1,\n",
    "    )\n",
    "    print(\"GPU usage:\\n{}\".format(gpu_df))\n",
    "    gpu_df[\"memory.free\"] = gpu_df[\"memory.free\"].map(\n",
    "        lambda x: int(x.rstrip(\" MiB\"))\n",
    "    )\n",
    "    idx = gpu_df[\"memory.free\"].idxmax()\n",
    "    print(\n",
    "        \"Returning GPU{} with {} free MiB\".format(\n",
    "            idx, gpu_df.iloc[idx][\"memory.free\"]\n",
    "        )\n",
    "    )\n",
    "    return idx\n",
    "\n",
    "\n",
    "cmd = \"export CUDA_VISIBLE_DEVICES=1,2,3,4,5,6\"\n",
    "os.popen(cmd)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    free_gpu_id = get_free_gpu()\n",
    "    print(f\"using GPU id: {free_gpu_id}\")\n",
    "    torch.cuda.set_device(free_gpu_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "539ace1c-6e23-41b2-93bb-2c27c6c7a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "43d7084a-36ce-4df6-82fc-eadf03524948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "with open('../data/train.history_belief') as fp:\n",
    "    raw_train_data = [line.split() for line in fp.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d1b3a76d-0c08-4943-8dc4-c340c5fe83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = torchtext.vocab.build_vocab_from_iterator(raw_train_data, specials=[\"<unk>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4c0865e2-bd47-4f03-997f-8709be79b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.set_default_index(train_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53565f97-e1ce-4bed-92f8-925b4e53d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "882f3874-21ee-4498-a1de-555fd2a2dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [torch.tensor(train_vocab(sent)[::-1], dtype=torch.long) for sent in raw_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c842d369-379a-4024-8802-2935e14dc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequence(train_data, padding_value=train_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cd4a16d3-f80c-4af3-ae14-665048a1138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.flip(train_data, (0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "58f97eab-e96d-4db1-b7f5-9359d38f5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    ntoken=len(train_vocab),\n",
    "    ninp=512,\n",
    "    nhead=2,\n",
    "    nhid=200,\n",
    "    nlayers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "df7d6ad4-522d-40f1-b640-95e74723755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-7038d532bc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-710e95e6f4bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, has_mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/NLP220/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 20\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "model.train()\n",
    "total_loss = 0.\n",
    "start_time = time.time()\n",
    "ntokens = len(train_vocab)\n",
    "for _ in tqdm(range(n_epochs), total=n_epochs):\n",
    "    for batch_idx in range(0, len(train_data), batch_size):\n",
    "        data = train_data[:, batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        data, targets = data[:-1, :], data[1:, :]\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.transpose(2, 1), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss / 10\n",
    "            elapsed = time.time() - start_time\n",
    "            print('ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                batch,\n",
    "                elapsed * 1000 / 10, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f2b7b4c5-9169-458e-96f2-798ffbe2e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test.history_belief') as fp:\n",
    "    raw_test_data = [line.split() for line in fp.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a6e498af-eb6f-4ccc-83de-13d9d43d2daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EOT = '<|endoftext|>'\n",
    "\n",
    "def translate(indexes):\n",
    "    indexes = list(indexes)\n",
    "    try:\n",
    "        found = indexes.index(EOT)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    else:\n",
    "        indexes = indexes[:found+1]\n",
    "    return train_vocab.lookup_tokens(list(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "212e5604-6681-499c-8aac-37c56b9da8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SOS = '<|context|>'\n",
    "INPUT_EOS = '<|endofcontext|>'\n",
    "OUTPUT_SOS = '<|belief|>'\n",
    "OUTPUT_EOS = '<|endofbelief|>'\n",
    "\n",
    "def belief_to_state_list(belief):\n",
    "    belief_list = [token for token in belief if token not in [OUTPUT_SOS, OUTPUT_EOS]]\n",
    "    belief_list = [slot.split() for slot in ' '.join(belief_list).split(',')]\n",
    "    return belief_list\n",
    "\n",
    "def belief_to_state_dict(belief):\n",
    "    belief_list = belief_to_state_list(belief)\n",
    "    state_dict = {}\n",
    "    for state in belief_list:\n",
    "        if len(state) < 3: continue\n",
    "        domain = state[0]\n",
    "        slot = state[1]\n",
    "        sub_slot = None\n",
    "        rest = state[2:]\n",
    "        if slot == 'book':\n",
    "            sub_slot = state[2]\n",
    "            rest = state[3:]\n",
    "        value = ' '.join(rest)\n",
    "        d = state_dict.get(domain, {})\n",
    "        if sub_slot:\n",
    "            ss = d.get(slot, {})\n",
    "            ss.update({\n",
    "                sub_slot: value\n",
    "            })\n",
    "            d.update({slot: ss})\n",
    "        else:\n",
    "            d.update({slot: value})\n",
    "        state_dict.update({domain: d})\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def match_slot(true, pred):\n",
    "    pred_state = belief_to_state_dict(pred)\n",
    "    true_list = belief_to_state_list(true)\n",
    "    slot_matches = []\n",
    "    for i, state in enumerate(true_list):\n",
    "        if len(state) < 3: continue\n",
    "        slot_matches.append(False)\n",
    "        domain = state[0]\n",
    "        if domain not in pred_state.keys(): continue\n",
    "        \n",
    "        slot = state[1]\n",
    "        if slot not in pred_state[domain].keys(): continue\n",
    "        \n",
    "        if slot != 'book':\n",
    "            true_value = \" \".join(state[2:])\n",
    "            pred_value = pred_state[domain][slot]\n",
    "        else:\n",
    "            sub_slot = state[2]\n",
    "            if sub_slot not in pred_state[domain][slot]: continue\n",
    "            true_value = \" \".join(state[3:])\n",
    "            pred_value = pred_state[domain][slot][sub_slot]\n",
    "        \n",
    "        if true_value != pred_value: continue\n",
    "        slot_matches[i] = True\n",
    "            \n",
    "    all_match = sum(slot_matches) == len(true_list)\n",
    "    \n",
    "    return all_match, slot_matches\n",
    "\n",
    "def get_accuracy(results):\n",
    "    total_states = len(results)\n",
    "    total_slots = sum([len(result[1]) for result in results])\n",
    "    total_correct_states = sum([result[0] for result in results])\n",
    "    total_correct_slots = sum([sum(result[1]) for result in results])\n",
    "    return {\n",
    "        'joint_accuracy': total_correct_states / total_states,\n",
    "        'slot_accuracy': total_correct_slots / total_slots\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e81ed3f-4e01-43d2-affe-ca347cfddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e9faedef-a592-4290-a0af-c0011f447740",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELIEF = '<|belief|>'\n",
    "\n",
    "test_data = []\n",
    "for raw_sent in raw_test_data[:2]:\n",
    "    indexes = train_vocab(raw_sent)\n",
    "    belief_idx = indexes.index(train_vocab[BELIEF])\n",
    "    data, target = indexes[:belief_idx][::-1], indexes[belief_idx:]\n",
    "    test_data.append((torch.tensor(data, dtype=torch.long),\n",
    "                     torch.tensor(target, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "628e3fe4-f487-4cb2-aae2-1f9ad3f57c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = pad_sequence([t[0] for t in test_data], padding_value=train_vocab['<pad>'])\n",
    "test_inputs = torch.flip(test_inputs, dims=(0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5820196a-c8ab-4f30-89ad-35cdc5038dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beliefs = [t[1] for t in test_data]\n",
    "max_test_target_len = max(len(t[1]) for t in test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "9ca073cc-e4a7-4e1c-8696-19546e4267f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 22.04it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = [torch.ones(test_inputs.shape[1], dtype=torch.long) * train_vocab[BELIEF]]\n",
    "input_data = torch.cat([test_inputs, torch.ones(1, test_inputs.shape[1], dtype=torch.long) * train_vocab[BELIEF]]).to(device)\n",
    "model.eval()\n",
    "for _ in tqdm(range(max_test_target_len), total=max_test_target_len):\n",
    "    out = model(input_data)\n",
    "    predictions.append(torch.argmax(out[-1], axis=1))\n",
    "    input_data = torch.cat([input_data, predictions[-1].unsqueeze(dim=0)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "73af9d29-ca66-43b7-ac17-54f677230ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a33425be-4100-4445-816f-6ef997f5e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for pred, target in zip(predictions.T, test_beliefs):\n",
    "    result = match_slot(translate(target), translate(pred))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a809e16f-2bb7-4b2a-b911-4627dcbe7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_accuracy': 0.0, 'slot_accuracy': 0.0}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1fac3-e05f-44f2-8d45-0464844c9f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b6111-7350-4a3f-846a-ac5553052590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP220",
   "language": "python",
   "name": "nlp220"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
